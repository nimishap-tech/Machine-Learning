{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder , OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.mixture import GaussianMixture as GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Data from the file \n",
    "df_data=pd.read_csv(\"C:/Users/PC/Desktop/ML/sgemm_product.csv\")\n",
    "df_data.head()\n",
    "\n",
    "###Taking average log values for the Last four attributes.\n",
    "df_data[\"Avg_run\"]=df_data.iloc[:,14:].mean(axis=1)\n",
    "df_data[\"log_avg_run\"]=np.log(df_data[\"Avg_run\"])\n",
    "df_data.drop(['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)','Avg_run'],axis=1,inplace=True)\n",
    "df_data.head()\n",
    "\n",
    "###Performing binary classification of data.\n",
    "median=df_data.log_avg_run.median()\n",
    "median\n",
    "##Splitting data over median value.\n",
    "X=df_data.iloc[:,1:14]\n",
    "Y=df_data.iloc[:,14].apply(lambda x : 0 if x <= median else 1)\n",
    "Y.value_counts()\n",
    "###Performing standardization\n",
    "\n",
    "X_s=X.copy()\n",
    "standardscalar=StandardScaler()\n",
    "X_s=standardscalar.fit_transform(X_s)\n",
    "\n",
    "#########Spliting into Train-Test######################\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_s,Y,test_size = 0.25 , random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using elblow method to find optimal number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range (1,30):\n",
    "    kmeans = KMeans(n_clusters=i ,  \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X_s)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,30),wcss,marker='o')\n",
    "plt.grid()\n",
    "plt.title(\"Error Rate-Elbow Method\")\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "#####Applying K-means algorithm\n",
    "kmeans = KMeans(n_clusters= 10, \n",
    "                init = \"k-means++\" , \n",
    "                max_iter= 500 , \n",
    "                n_init= 10 , \n",
    "                random_state= 0)\n",
    "\n",
    "y_kmeans = kmeans.fit_predict(X_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENTIONALITY REDUCTION-PCA/ICA/RP/FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing PCA for Energy dataset to find maximum variance \n",
    "\n",
    "pca = PCA(n_components= None)\n",
    "X_PCA = pca.fit_transform(X_s)\n",
    "#X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "plt.plot(range(1,14),explained_variance,marker='o')\n",
    "plt.title(\"PCA Variance\")\n",
    "plt.xlabel(\"Component Number\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "###Choosing Optimal n-components\n",
    "pca_final_energy = PCA(n_components =10)\n",
    "X_PCA= pca_final_energy.fit_transform(X_s)\n",
    "\n",
    "#####Plotting error v/s PCA\n",
    "wcss_pca = []\n",
    "for i in range (1,20):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X_PCA)\n",
    "    wcss_pca.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,20),wcss_pca,marker='o')\n",
    "plt.title(\"PCA-Elbow Method\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Implementing ICA\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "ica = FastICA(tol = 1e-2, max_iter = 1000, n_components =10)\n",
    "X_ICA = ica.fit_transform(X_s)\n",
    "\n",
    "wcss_energy_ica = []\n",
    "for i in range (1,21):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X_ICA)\n",
    "    wcss_energy_ica.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,21),wcss_energy_ica,marker='o')\n",
    "plt.title(\"ICA-Elbow Method\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Implementing RP\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "srp = SparseRandomProjection(eps = 0.1, n_components = 10)\n",
    "X_RP = srp.fit_transform(X_s)\n",
    "\n",
    "wcss_energy_ra = []\n",
    "for i in range (1,11):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X_RP)\n",
    "    wcss_energy_ra.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11),wcss_energy_ra,marker='o')\n",
    "plt.title(\"Randomized Projection-Elbow Method\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Implementing Feature Selection\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', max_depth=6)\n",
    "rfecv = RFECV(estimator=dt, step=1, cv=5, n_jobs=-1,\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X,Y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(rfecv.ranking_)\n",
    "\n",
    "###Plotting Features\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.grid()\n",
    "plt.title(\"Feature scores\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_,marker='o')\n",
    "plt.show()\n",
    "\n",
    "####Plotting error v/s feature\n",
    "\n",
    "X_FS = X_s[:,(0,2,3,7)]\n",
    "\n",
    "wcss_FS = []\n",
    "for i in range (1,20):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X_FS)\n",
    "    wcss_FS.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,20),wcss_FS,marker='o')\n",
    "plt.title(\"Feature Selection-Elbow Method \")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING ANN USING DIMENTIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ANN CLASSIFIER\n",
    "\n",
    "#####Final ANN Classifier\n",
    "\n",
    "ANN_class= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_class.fit(X_train, y_train)\n",
    "#F\n",
    "###Predicting\n",
    "y_pred= ANN_class.predict(X_test)\n",
    "y_expect = y_test\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1 = accuracy_score(y_expect , y_pred)\n",
    "print(acc_dt1)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1= confusion_matrix(y_test , y_pred)\n",
    "print(cm_dt1)\n",
    "\n",
    "##ANN Classifier using PCA\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_PCA,Y,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_PCAclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_PCAclass.fit(X_train, y_train)\n",
    "\n",
    "###Predicting\n",
    "y_pred_pca= ANN_PCAclass.predict(X_test)\n",
    "y_expect_pca = y_test\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1_pca = accuracy_score(y_expect_pca , y_pred_pca)\n",
    "print(acc_dt1_pca)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1_pca= confusion_matrix(y_test , y_pred_pca)\n",
    "print(cm_dt1_pca)\n",
    "\n",
    "###ANN Classifier using ICA\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_ICA,Y,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_ICAclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_ICAclass.fit(X_train, y_train)\n",
    "\n",
    "###Predicting\n",
    "y_pred_ica= ANN_ICAclass.predict(X_test)\n",
    "y_expect_ica = y_test\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1_ica = accuracy_score(y_expect_ica , y_pred_ica)\n",
    "print(acc_dt1_ica)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1_ica= confusion_matrix(y_test , y_pred_ica)\n",
    "print(cm_dt1_ica)\n",
    "\n",
    "###ANN Classifier using RP\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_RP,Y,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_PCAclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_class.fit(X_train, y_train)\n",
    "\n",
    "###Predicting\n",
    "y_pred_rp= ANN_class.predict(X_test)\n",
    "y_expect_rp = y_test\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1_rp = accuracy_score(y_expect_rp, y_pred_rp)\n",
    "print(acc_dt1_rp)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1_rp= confusion_matrix(y_test , y_pred_rp)\n",
    "print(cm_dt1_rp)\n",
    "\n",
    "###ANN Classifier using FS\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_FS,Y,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_FSclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_FSclass.fit(X_train, y_train)\n",
    "\n",
    "###Predicting\n",
    "y_pred_fs= ANN_FSclass.predict(X_test)\n",
    "y_expect_fs = y_test\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1_fs = accuracy_score(y_expect_fs, y_pred_fs)\n",
    "print(acc_dt1_fs)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1_fs= confusion_matrix(y_test , y_pred_fs)\n",
    "print(cm_dt1_fs)\n",
    "\n",
    "###Plotting Accuraires\n",
    "\n",
    "accuracies_energy = [acc_dt1 ,acc_dt1_pca,acc_dt1_rp,acc_dt1_ica,acc_dt1_fs]\n",
    "accuracies_type = ['Normal','PCA','Random Project','ICA','Feature Selection']\n",
    "plt.plot(accuracies_type , accuracies_energy , marker='o',color='blue')\n",
    "plt.title(\" Accuracies for different Dimension Reduction\")\n",
    "plt.xlabel(\"Reduction Techniques\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPECTATION MAXIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bic = []\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "n_components = np.arange(1,11,1)\n",
    "for i in cv_types:\n",
    "    bic_cv = []\n",
    "    for n in n_components:\n",
    "        EM_Model = GMM(n_components=n, covariance_type=i, n_init=1)\n",
    "        EM_Model.fit(X_FS)\n",
    "        bic_cv.append(EM_Model.bic(X_FS)) ##Run same code by replacing X_FS,X_PCA,X_RP,X_ICA,X_s\n",
    "    bic.append(bic_cv)\n",
    "    \n",
    "for i in range(0,len(cv_types)):\n",
    "    plt.plot(n_components, bic[i], marker='o', label=\"Covariance Type = {}\".format(cv_types[i]))\n",
    "    plt.legend(fontsize='small')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of Clusters', fontsize=14)\n",
    "    plt.ylabel('BIC score', fontsize=14)\n",
    "    plt.title('BIC score - Dataset 1', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN CLASSIFIER WITH OUTPUT CLASS LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(y_kmeans ,Y,test_size = 0.25 , random_state = 0)\n",
    "ANN_class= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_class.fit(X_train.reshape(-1,1), y_train)\n",
    "#F\n",
    "###Predicting\n",
    "y_pred= ANN_class.predict(X_test.reshape(-1,1))\n",
    "y_expect = y_test\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1 = accuracy_score(y_expect , y_pred)\n",
    "print(acc_dt1)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1= confusion_matrix(y_test , y_pred)\n",
    "print(cm_dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Dataset2\n",
    "\n",
    "df_data2=pd.read_csv(\"C:/Users/PC/Desktop/ML/Bank_Data.csv\")\n",
    "df_data2.head()\n",
    "\n",
    "##Converting Variables to Categorical Data.\n",
    "\n",
    "df_data2['default']=df_data2['default'].map({'unknown':0,'no':1,'yes':2})\n",
    "df_data2['housing']=df_data2['housing'].map({'unknown':0,'no':1,'yes':2})\n",
    "df_data2['loan']=df_data2['loan'].map({'unknown':0,'no':1,'yes':2})\n",
    "df_data2['month']=df_data2['month'].map({'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12})\n",
    "df_data2['poutcome']=df_data2['poutcome'].map({'nonexistent':0,'failure':1,'success':2})\n",
    "df_data=df_data2.drop(['duration'],axis=1)\n",
    "#df_data2.head()\n",
    "\n",
    "###Converting Nominal Variable.\n",
    "\n",
    "collst=['job','marital','education','contact','day_of_week']\n",
    "df_data2=pd.get_dummies(df_data2,columns=collst)\n",
    "\n",
    "df_data2['y']=df_data2['y'].map({'yes':1,'no':0})\n",
    "\n",
    "####Splitting Data into Train Test\n",
    "X2=df_data2.drop('y',axis=1).values\n",
    "Y2=df_data2['y'].values\n",
    "\n",
    "\n",
    "####Splitting data into train test\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2,Y2,test_size = 0.25 , random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using elblow method to find optimal number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range (1,30):\n",
    "    kmeans = KMeans(n_clusters=i ,  \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X2)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,30),wcss,marker='o')\n",
    "plt.grid()\n",
    "plt.title(\"Error Rate-Elbow Method\")\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "####K-MEANS\n",
    "kmeans = KMeans(n_clusters= 4, \n",
    "                init = \"k-means++\" , \n",
    "                max_iter= 500 , \n",
    "                n_init= 10 , \n",
    "                random_state= 0)\n",
    "\n",
    "y_kmeans = kmeans.fit_predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING DIMENSION REDUCTIONALITY PCA/ICA/RP/FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing PCA for Energy dataset to find maximum variance \n",
    "\n",
    "pca = PCA(n_components= None)\n",
    "X2_PCA = pca.fit_transform(X2)\n",
    "#X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "plt.plot(range(1,47),explained_variance,marker='o')\n",
    "plt.title(\"PCA Variance\")\n",
    "plt.xlabel(\"Component Number\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "pca_final_energy = PCA(n_components =2)\n",
    "X2_PCA= pca_final_energy.fit_transform(X2)\n",
    "\n",
    "wcss_pca = []\n",
    "for i in range (1,20):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X2_PCA)\n",
    "    wcss_pca.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,20),wcss_pca,marker='o')\n",
    "plt.title(\"PCA-Elbow Method\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "ica = FastICA(tol = 1e-2, max_iter = 1000, n_components =2)\n",
    "X2_ICA = ica.fit_transform(X2)\n",
    "\n",
    "wcss_energy_ica = []\n",
    "for i in range (1,21):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X2_ICA)\n",
    "    wcss_energy_ica.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,21),wcss_energy_ica,marker='o')\n",
    "plt.title(\"ICA-Elbow Method\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import SparseRandomProjection\n",
    "srp = SparseRandomProjection(eps = 0.1, n_components = 2)\n",
    "X2_RP = srp.fit_transform(X2)\n",
    "\n",
    "wcss_energy_ra = []\n",
    "for i in range (1,11):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X2_RP)\n",
    "    wcss_energy_ra.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11),wcss_energy_ra,marker='o')\n",
    "plt.title(\"Randomized Projection-Elbow Method\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Feature Selection\n",
    "\n",
    "\n",
    "# Using RFE to do the feature selection by Random Forest\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', max_depth=6)\n",
    "rfecv = RFECV(estimator=dt, step=1, cv=5, n_jobs=-1,\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X2,Y2)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "print(rfecv.ranking_)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score\")\n",
    "plt.grid()\n",
    "plt.title(\"Feature scores\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_,marker='o')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X2_FS = X2[:,(2,5)]\n",
    "\n",
    "wcss_FS = []\n",
    "for i in range (1,20):\n",
    "    kmeans = KMeans(n_clusters=i , \n",
    "                    init = \"k-means++\" , \n",
    "                    max_iter= 500 , \n",
    "                    n_init= 10 , \n",
    "                    random_state= 0 )\n",
    "    kmeans.fit(X2_FS)\n",
    "    wcss_FS.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,20),wcss_FS,marker='o')\n",
    "plt.title(\"Feature Selection-Elbow Method \")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Number of Cluster\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING ANN WITH DIMENTIONALIY REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ANN\n",
    "\n",
    "####FINAL CLASSIFIER A-NN####\n",
    "\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2,Y2,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_class= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'adam',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 300,\n",
    "                          random_state= 0 )\n",
    "ANN_class.fit(X_train2, y_train2)\n",
    "\n",
    "##Making Prediction\n",
    "y2_pred = ANN_class.predict(X_test2)\n",
    "y2_expect = y_test2\n",
    "\n",
    "### Accuracy\n",
    "acc_dt2 = accuracy_score(y2_expect , y2_pred)\n",
    "print(acc_dt2)\n",
    "\n",
    "##Confusion matrix\n",
    "cm_dt2 = confusion_matrix(y_test2 , y2_pred)\n",
    "print(cm_dt2)\n",
    "\n",
    "###PCA\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2_PCA,Y2,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_PCAclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'adam',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 300,\n",
    "                          random_state= 0 )\n",
    "ANN_PCAclass.fit(X_train2, y_train2)\n",
    "\n",
    "##Making Prediction\n",
    "y2_pred = ANN_PCAclass.predict(X_test2)\n",
    "y2_expect = y_test2\n",
    "\n",
    "### Accuracy\n",
    "acc_dt2_pca = accuracy_score(y2_expect , y2_pred)\n",
    "print(acc_dt2_pca)\n",
    "\n",
    "##Confusion matrix\n",
    "cm_dt2_pca = confusion_matrix(y_test2 , y2_pred)\n",
    "print(cm_dt2_pca)\n",
    "\n",
    "##ICA\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2_ICA,Y2,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_ICAclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'adam',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 300,\n",
    "                          random_state= 0 )\n",
    "ANN_ICAclass.fit(X_train2, y_train2)\n",
    "\n",
    "##Making Prediction\n",
    "y2_pred = ANN_ICAclass.predict(X_test2)\n",
    "y2_expect = y_test2\n",
    "\n",
    "### Accuracy\n",
    "acc_dt2_ica = accuracy_score(y2_expect , y2_pred)\n",
    "print(acc_dt2_ica)\n",
    "\n",
    "##Confusion matrix\n",
    "cm_dt2_ica = confusion_matrix(y_test2 , y2_pred)\n",
    "print(cm_dt2_ica)\n",
    "\n",
    "####FINAL CLASSIFIER A-NN-RP####\n",
    "\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2_RP,Y2,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_RPclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'adam',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 300,\n",
    "                          random_state= 0 )\n",
    "ANN_RPclass.fit(X_train2, y_train2)\n",
    "\n",
    "##Making Prediction\n",
    "y2_pred = ANN_RPclass.predict(X_test2)\n",
    "y2_expect = y_test2\n",
    "\n",
    "### Accuracy\n",
    "acc_dt2_rp = accuracy_score(y2_expect , y2_pred)\n",
    "print(acc_dt2_rp)\n",
    "\n",
    "##Confusion matrix\n",
    "cm_dt2_rp = confusion_matrix(y_test2 , y2_pred)\n",
    "print(cm_dt2_rp)\n",
    "\n",
    "####FS\n",
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(X2_FS,Y2,test_size = 0.25 , random_state = 0)\n",
    "\n",
    "ANN_FSclass= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'adam',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 300,\n",
    "                          random_state= 0 )\n",
    "ANN_FSclass.fit(X_train2, y_train2)\n",
    "\n",
    "##Making Prediction\n",
    "y2_pred = ANN_FSclass.predict(X_test2)\n",
    "y2_expect = y_test2\n",
    "\n",
    "### Accuracy\n",
    "acc_dt2_fs = accuracy_score(y2_expect , y2_pred)\n",
    "print(acc_dt2_fs)\n",
    "\n",
    "##Confusion matrix\n",
    "cm_dt2_fs = confusion_matrix(y_test2 , y2_pred)\n",
    "print(cm_dt2_fs)\n",
    "\n",
    "###Plotting Accuraires\n",
    "\n",
    "accuracies_energy = [acc_dt2 ,acc_dt2_pca,acc_dt2_rp,acc_dt2_ica,acc_dt2_fs]\n",
    "accuracies_type = ['Normal','PCA','Random Project','ICA','Feature Selection']\n",
    "plt.plot(accuracies_type , accuracies_energy , marker='o',color='blue')\n",
    "plt.title(\" Accuracies for different Dimension Reduction\")\n",
    "plt.xlabel(\"Reduction Techniques\")\n",
    "plt.grid()\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPECTATION MAXIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = []\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "n_components = np.arange(1,11,1)\n",
    "for i in cv_types:\n",
    "    bic_cv = []\n",
    "    for n in n_components:\n",
    "        EM_Model = GMM(n_components=n, covariance_type=i, n_init=2)\n",
    "        EM_Model.fit(X2)\n",
    "        bic_cv.append(EM_Model.bic(X2))\n",
    "    bic.append(bic_cv)\n",
    "   \n",
    "##Plotting\n",
    "for i in range(0,len(cv_types)):\n",
    "    plt.plot(n_components, bic[i], marker='o', label=\"Covariance Type = {}\".format(cv_types[i]))\n",
    "    plt.legend(fontsize='small')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of Clusters', fontsize=14)\n",
    "    plt.ylabel('BIC score', fontsize=14)\n",
    "    plt.title('BIC score - Dataset 2', fontsize=14)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN CLASSIFIER WITH OUTPUT LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2,X_test2, y_train2, y_test2 = train_test_split(y_kmeans,Y2,test_size = 0.25 , random_state = 0)\n",
    "ANN_class= MLPClassifier(hidden_layer_sizes= (30,30,30,30),\n",
    "                          activation='relu',  \n",
    "                          batch_size='auto' ,\n",
    "                          solver= 'sgd',\n",
    "                          learning_rate= 'constant',\n",
    "                          max_iter= 500,\n",
    "                          random_state= 0 )\n",
    "ANN_class.fit(X_train2.reshape(-1,1), y_train2)\n",
    "#F\n",
    "###Predicting\n",
    "y_pred= ANN_class.predict(X_test2.reshape(-1,1))\n",
    "y_expect = y_test2\n",
    "\n",
    "##Accuracy\n",
    "acc_dt1 = accuracy_score(y_expect , y_pred)\n",
    "print(acc_dt1)\n",
    "\n",
    "### create confusion matrix\n",
    "cm_dt1= confusion_matrix(y_test2 , y_pred)\n",
    "print(cm_dt1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
